{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken  # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Analysis for GPT-4o Mini Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 1173\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are an assistant for question-answering tasks about amazon web service.'}\n",
      "{'role': 'user', 'content': 'Q: What is Amazon EC2 Auto Scaling?'}\n",
      "{'role': 'assistant', 'content': 'Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application. Amazon EC2 Auto Scaling helps you maintain application availability through fleet management for EC2 instances, which detects and replaces unhealthy instances, and by scaling your Amazon EC2 capacity up or down automatically according to conditions you define. You can use Amazon EC2 Auto Scaling to automatically increase the number of Amazon EC2 instances during demand spikes to maintain performance and decrease capacity during lulls to reduce costs.'}\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "data_path = \"data.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if messages is None:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "    else:\n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "            if message.get(\"role\") not in (\"system\", \"user\", \"assistant\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "            if not isinstance(message.get(\"content\", ''), str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "        if not any(message.get(\"role\") == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Counting Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += len(encoding.encode(message['content']))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.05)}, {np.quantile(values, 0.95)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Warnings and Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 36, 972\n",
      "mean / median: 125.70332480818415, 109.0\n",
      "p5 / p95: 49.0, 264.7999999999997\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 4, 946\n",
      "mean / median: 95.42540494458653, 78.0\n",
      "p5 / p95: 18.0, 234.0\n"
     ]
    }
   ],
   "source": [
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    system_present = any(m[\"role\"] == \"system\" for m in messages)\n",
    "    user_present = any(m[\"role\"] == \"user\" for m in messages)\n",
    "    n_missing_system += not system_present\n",
    "    n_missing_user += not user_present\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_tokens_from_messages([m for m in messages if m[\"role\"] == \"assistant\"]))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~147450 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~442350 tokens\n"
     ]
    }
   ],
   "source": [
    "# Assume each example's token count does not exceed the maximum context length for the model.\n",
    "n_epochs = 3  # typically a good starting point\n",
    "n_train_examples = len(dataset)\n",
    "n_billing_tokens_in_dataset = sum(l for l in convo_lens)\n",
    "\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading a Training File for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "file_upload_response = client.files.create(\n",
    "    file=open(\"data.jsonl\", \"rb\"),  # Make sure the file path and name are correct\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(file_upload_response)  # This will print the response from the server including the file ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-1QPUJpBQzVQbHQO7G9qu9Iml', created_at=1723569390, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-LUz8bhCnZYA1TqgxH6GUEo06', result_files=[], seed=2087838318, status='validating_files', trained_tokens=None, training_file='file-Mnvub7g7KEiJ5LSfOU8yLkam', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix='awsage')\n"
     ]
    }
   ],
   "source": [
    "# Following code will start the finetuning job which you can check in the OpenAI Dashboard\n",
    "fine_tuning_response = client.fine_tuning.jobs.create(\n",
    "    training_file=file_upload_response.id,  # File id from the above response\n",
    "    model=os.environ['OPENAI_FINETUNING_MODEL'],\n",
    "    hyperparameters={'n_epochs':n_epochs},\n",
    "    suffix=os.environ['OPENAI_FINETUNING_MODEL_SUFFIX']\n",
    ")\n",
    "print(fine_tuning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-1QPUJpBQzVQbHQO7G9qu9Iml', created_at=1723569390, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal:awsage:9vq7igpe', finished_at=1723572085, hyperparameters=Hyperparameters(n_epochs=3, batch_size=2, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-LUz8bhCnZYA1TqgxH6GUEo06', result_files=['file-cnBsuFqsoQDTOpComXJa0doR'], seed=2087838318, status='succeeded', trained_tokens=480009, training_file='file-Mnvub7g7KEiJ5LSfOU8yLkam', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix='awsage')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ft:gpt-4o-mini-2024-07-18:personal:awsage:9vq7igpe'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the state of a fine-tune job\n",
    "job_status = client.fine_tuning.jobs.retrieve(fine_tuning_response.id)\n",
    "print(job_status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
